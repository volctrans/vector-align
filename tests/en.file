Capabilities provided by SAMI\n \n*About Us\n*The SAMI (SAMI-Speech, Audio \u0026 Music Intelligence) team is responsible for the development of speech, audio, and music technologies and related product innovation. Our mission is to leverage multimodal audio technology for better content creation and interaction, making content consumption and generation easier, more entertaining and diverse. The team serves as a mid-platform and provides services to multiple business lines, supporting work related to, for example, speech synthesis, audio processing and analysis, music analysis and creation.\n*Contact List\n*Contact List for Product Lines\n* Reach out to  if you don't know who to contact.\n**\n*List of Capabilities\nList of capabilities-WIP:\n*SAMI list of capabilities (WIP)\n*Speech Synthesis:\n*Product requirement POC:  @Wendi Liu\n*Technical POC: @ÁéãÁéâÂπ≥\n*Introduciton to ByteTTS\n*\n*üí° Tips: What is Speech Synthesis? üí°\n*Speech synthesis basically means speech generation, we can generate speech from text, that is TTS, and also can generate speech from speech, which is Voice conversion. TTS technology converts the text input to audio and is commonly used in intelligent customer service and text reading scenarios. Our TTS technology now supports multiple languages and is used in various music and singing scenarios.\n*TTS - Chinese, multi-language, singing\n*Introduction\n*The TTS algorithm identifies the numbers, polyphones in the text, auto-punctuates the text, and produces an audio version or a singing version of the text with high precision.\n*TTS for other languages apart from Chinese is in the pipeline. We now support: English, Japanese, Korean, Brazilian Portuguese, Spanish, Indonesia; WIP: Italian, French, Russian etc.\n*Business value\n*Lower the threshold for video-making and improve content quality. Video content quality and posting efficiency are improved as the Chinese text reading feature for Douyin incorporates TTS.\n*Offer natural and smooth voice delivery. With the most advanced E-to-E TTS technology and deep neural vocoder solution, the team now delivers natural, realistic, and expressive voices. Now, over dozens of voices can be played in real time as the synthesis is performed.\n*Be available for multiple languages and dialects. TTS now supports various languages and dialects with clear, standard, and highly realistic deliveries. TTS for Chinese supports Chinese-English switching for all voices.\n*Allow customization. Users can have their customized voice easily generated by adjusting various parameters like speed and tone in real time. A small voice sample will be enough as input. \n*Application scenario\n*Reading aloud: TTS is used for reading novels, news, ads, and other texts with distinctive styles and rich emotions. Text reading features of Douyin and TikTok have incorporated this technology.\n*Games: TTS allows customized NPC voicers and unique speech packages.\n*Virtual characters: TTS produces specific voices catered to virtual characters, allows lip-syncing with the correct time stamps, delivering realistic user experience.\n*Audio/video editing: TTS provides diversified voice choices for dubbing and transition, enriching the creation experience.\n*Customized business campaign slogans: TTS supports AI dubbing for various business slogans, making the videos more professional.\n*Demo\n*Customized experience: https://sami.bytedance.net/#/\n* \n**\n* \n \n*Singing voice synthesis\n*Introduction\n*With the singing voice synthesis feature, not only can texts be read, but they can also be sung with rhythm. Intelligent word segmentation can now be done based on the rhythm and pace of the music piece, making music creation and singing easier.\n*We introduce Gan vocoder into singing model which delivered even more realistic singing and very high inference efficiency\n*Singing voice synthesis is available for Chinese, English, and several others.\n*Business value\n*Upgrade AI music generation capability and quality for more lively and expressive music pieces.\n*Lower the threshold and help users create better music, promote the concept of \"Anyone can be a musician\" to engage more users in content creation.\n*Curb music creation cost relating to, for example, music recording and audio mixing, as AI singing voice synthesis produces music pieces with higher sound quality.\n*Yield more room for individual creativity and allow higher engagement and flexibility. Users are given a wide range of choices such as editing music content and adjusting singing voices, creating their OWN music.\n*Application scenario\n*Audio/Video platform: The technology helps with better creation by enabling props and stickers, thus meeting specific singing creation demands for Douyin and TikTok etc.\n*Content creation: Users can now substitute lyrics with their original texts, or use lyrics of one song for another song, which leads to more possibilities with music derivatives.\n*Creation for fun: As all texts now can be sung, this has unlocked another channel for promoting literature works. \n*Customized music creation for business: Texts can be automatically turned into lyrics and songs can be auto-generated, saving time and cost of music creation for promotion purposes.\n*Demo\n* \n \n \n*Meme song: https://sami.bytedance.net/#/capability/MemeSongDemo\n \n*Voice conversion and singing voice conversion\n*Introduction\n*Voice conversion is when the same text is read by a different voice, producing an effect similar to that of Detective Conan's tie.\n*Moreover, we can convert the voice of a particular song. Specific algorithms can perform sophisticated pitch control and clone the voice of another speaker to one particular song. With a speech recording of the targeted voice, we can easily change the voice of a song to the desired voice.\n*Business value\n*Lower the threshold for video shooting, improve video content quality and level of engagement.\n*Yield more room for individual creativity and allow higher engagement and flexibility. Users are given a wide range of choices such as editing music content and adjusting singing voices, creating their OWN music.\n*Enrich audio/video content to cater to the needs of specific scenarios. Users can select from a wide variety of timbre to better echo their audio/video content.\n*Application scenario\n*Short video/ads dubbing: Allow users to choose voices for dubbing and enjoy a more engaging experience.\n*Influencer monetization: Earn money by performing dubbing on the platform which offers a vast selection of voices.\n*Social interaction and entertainment: Enrich interactive experience by enabling various voice choices.\n*ÊïàÊûúÂ±ïÁ§∫ Demo\n* \n*\n*Êõ¥Â§öÊ≠åÂ£∞ËΩ¨Êç¢ÔºöVoice changing for songs\n*https://www.bilibili.com/audio/am31538610?type=6\n*https://www.bilibili.com/audio/am31562539?type=7 \n*\n*Voice cloning/Cross-language voice cloning\n*Introduction\n*Just with a 2 minute-sample recorded in a traditional recording environment, voice cloning can deliver soundtracks of the speaker that are highly precise and natural in a consistent manner. The technology can even generate soundtracks of the speaker speaking different languages.\n*Voice navigation of celebrities in map applications is developed with the help of voice cloning.\n*Business value\n*Lead to endless application possibilities, such as more engaging video shooting and the potential usage in intelligent softwares and hardwares (e.g. celebrity navigation).\n*Enable natural and consistent delivery with results highly realistic and similar to the original.\n*Allow greater room for customization with desirable voices generated for specific scenarios, boosting text-reading performance.\n* Promote the speaker to become more well-known internationally through cross-language voice cloning, creating \"international voice IPs\".\n*Application scenario\n*Customized voice: Efficiently deliver solutions of customized voices with high quality for all relevant scenarios.\n*Audio/video editing: Allow users to have their personalized voices which inspires audio/video creation\n*Social interaction: Allow users to change or save voices, making voice chat easier and more important in social interactions.\n*Smart device: Enhance emotional experience with a sense of personalization through voice-augmenting smart devices.\n*Promotion of audio content to foreign-language speakers: Enable better localization for audio content without having to reduce the content quality. For example, voice cloning can promote movies to foreign language speakers.\n*Demo\n*Customized celebrity voice (Huang Jianxiang)\n* \n*Result of voice cloning for the same language\n*\n* Cross-language voice cloning EN-\u003eCH\n*\n*\n* Music editing and auto-generation\n*PM     \n*Tech POC     \n*\n*üí° Tips: What is music editing and auto-generation? üí°\n*Simply put, with certain algorithms, it is now possible to edit and auto-generate music pieces, making creation easier.\n*Capabilities provided by SAMI can be applied to independent music pieces with clear-cut copyright information and great quality, as well as help users create music easily, facilitating music creation in together with video creation.\n*Music auto-generation with AI\n*Introduction\n*Intelligent algorithm can generate music that are catered to users' liking, reducing the threshold for music creation and production, and promoting the idea of \"Anyone can be a musician.\"\n*The algorithm supports customization of music with specific style, genre, and content.\n*Business value\n*Allow high flexibility and level of customization as the music copyright is more flexible with independent ownership.\n*Achieve high content productivity and low cost.\n*Support customization in various dimensions, such as style, language, emotion, and scenario;  Users can use AI, without having to feed it with specific information, to generate music of the same type they like.\n*Enrich creation experience by unleashing creation potential beyond the restrictions of music copyright.\n*Quickly auto-generate personalized music catered to users' preference.\n*Application scenario\n*Video background music: provide more choices for video background music that are more suitable considering the video content.\n*Background music of promotion ads : support all-channel ad marketing music selections, giving users more choices beyond the restriction of copyright.\n*Game music: offer low-cost customized music solutions for different styles of games.\n*Case-based background music: meet demand for background music under different scenarios, for example background music for TV show warm-up, and background music for events.\n*Demo\n*AI Music Style \n* \n*\n*Music effects\n*Introduction\n*With music effects, the team provides an interactive music experience for Douyin and TikTok users, and develop music generation algorithm and music understanding algorithm, supporting stickers and props related to music creation, gaming, visualization, and voice changing.\n*Business value\n*Provide innovative ways of engaging users with stickers and props to accentuate our advantage of audio-video combination. Works on music effect have been very creative, breaking the 2D visualization limit with traditional shooting tools. According to released data on prop engagement, audio-video tools are effective in encouraging dormant users to shoot videos.\n*High engagement level. Music effects enrich and improve social interaction and entertainment experience.\n*More engagement variations, including but not limited to possibilities with meter, melody, and video creation. For example, rhyme master, one-stop generation of rhythmic videos with head movements .\n*Application scenario\n*Audio/video editing: Available for Douyin and TikTok. Provides engaging music effects.\n*Music visualization: Improve user interactive experience as the technology helps with the natural combination of music and visual effects.\n*Music promotion: Per music content,  the algorithm allows users to choose different ways to play with videos, increasing the number of music video posts, reaching more users with our music.\n*Demo\n* \n*\n*MusicDSP SDK \n*Introduction\n*Enable real-time music generation on client and real-time editing, remixing, and rendering. E.g. Real-time remixing and pitch raise and reduction.\n*Business value\n*Support AI music generation, music effect and music creation.\n*High timeless, adaptivity and flexibility.\n*Application scenario\n*Music props and stickers: provide engaging stickers and props with high audio quality, improving the music interaction experience. Currently available for Douyin and TikTok.\n*Content creation: identify more scenarios related to music as the technology helps to analyze the melody, rhythm and other dimensions of music pieces, boosting the user interaction experience.\n*Audio effect enrichment: Intelligent editing, remix and rendering make the music piece more expressive.\n*Demo\n* \n* \nMusic generation using templates\n*Introduction\n*Provide various music templates where users only need to provide text input and the intelligent algorithm will adapt the texts with pre-set rhymes and produce a singing version.\n*Business value\n*Reduce threshold for music creation, enrich platform content and foster musical creation ecology, making the platform less reliant on PGC music.\n*Application scenario\n*The technology allows more content diversity, facilitating intelligent music creation and diversifying the use of other fun tools as well as campaign activities.\n*Fun text reading: Enrich the experience of text reading, bringing in more possibilities.\n*Demo\n* \n*Text to songs: auto-song generation\n*Introduction\n*We now support auto-song generation based on text input and singing performance display.\n*Business value\n*Reduce music production cost; fulfill music customization requests, specifically auto-song generation from texts; foster music creation ecology; making the platform less reliant on PGC music.\n*Application scenario\n*Music creation: offer a more efficient and convenient tool for music creation and inspire creativity with intelligent song generation technology, creating more possibilities for music derivatives.\n*Music customization: meet unique demand for music creativity where users can have their personalized music automatically generated from text.\n*Demo\n \n*\n*Intelligent Remix\n*Introduction\n*Offer one-click solution to change music style with intelligent music rearrangement.\n*Business value\n*Reduce threshold for music creation, enrich content on the platform, and foster music creation ecology.\n*Incorporate AI intelligence, striking a perfect balance between cost, efficiency, and content quality.\n*Application scenario\n*Music production: Intelligent remix helps to mix different voice signals, generating layered music.\n*Post-production editing: Handle voices with great sophistication and efficiency, improving music editing experience.\n*Demo\n*0716.test.xuemaojiao.remix.Piano.wav\n* \n*Understanding of multi-modal music\n*PM   \n*Tech POC   \n*üí° Tips: What is the understanding of multi-modal music? üí°\n*It refers to the interpretation and objective display of music content from various dimensions. With well-established tagging system such as by genre and by emotion, we can interpret and use music content based on music form, melody, concord and rhythm, ensuring high-quality content for the various possibilities of user engagement.\n*Music auto-tagging\n*Introduction\n*Music auto-tagging refers to the categorizations of music. Every set of tags corresponds to one music categorization method. For example, under music genre tags, there are Pop, Rock, Classical, etc. We can also tag music according to its language.\n*SAMI categorizes music by its genre, emotion, language, scenario, instrumental/lyrical, and audio feature. For more details, please refer to SAMI tagging white paper\n*Business value\n*Provide clear and easy-to-understand music categorization, facilitating content discovery and aggregation to meet different music consumption demand.\n*Support more features like tag searching, tag filtering, and song recommendation, achieving efficient and extensive application of music categorization.\n*Deep learning of billions of music pieces from hundreds of million songs in the music library ensures tagging accuracy, we now have an average of 94% accuracy for genre tagging, 93% for emotion tagging, 90% for scenario tagging, and 93.8% for language tagging.\n*Extremely short latency with a RTF of 0.04, indicating fast response rate that is very close to real time.\n*Application Scenario\n*Product lines: We are now offering services like tag searching, tag selection and song recommendation for Douyin, TikTok, Jianying, Xigua, and Resso and other businesses that require these features.\n*Music library software: Music tagging is mostly used in generating music library metadata,  through which libraries can store tags in the database for downstream applications. The service is already available for Resso music library categorization and playlist creation.\n*Audio/video editing software: Matching video/text contents with suitable background music requires a comprehensive set of tags.\n*Music recommendation and search: Compare music pieces based on their tags and recall music pieces with high similarity. Music tagging can leverage user data to recommend music pieces the user might like. The algorithm generates feature tags based on multiple music features, providing a solid foundation for music search and recommendation.\n*Marketing: The technology allows users to select tags catering for certain marketing demands.\n*Demo\n* \n\n*Beat detection\n*Introduction\n*Beat detection tells the beat of the music through automated analysis, helping creators to make better use of music resources.  Information on downbeat and upbeat helps creators make rhythmic videos, create vlogs and beat effect.\n*Beat detection can tell the type of meter, such as triple time, quadruple time, accent, and speed. For information mapping specification of BPM data and rhythm, please refer to BPM-speed mapping specification\n*Business value\n*Beat is the most distinctive feature users can perceive. When used in specific scenarios in combination with visual effects, beat detection can improve user experience for content consumption.\n*Beat detection, together with music tagging and similarity analysis, can help with the understanding of music.\n*Lower creation threshold and expand application scenarios. Beat detection algorithm automatically identifies the timing of beats, helping users to create high-quality rhythmic videos in combination with other automation tools.\n*High engagement level. Beat detection provides more choices for user interaction.\n*Application scenario\n*Props and stickers: Utilize music meter information to create visual effects for audios/videos.\n*Social interactions and entertainment: Design reference points for interactive games like dancing and singing, improving users' experience with content consumption. Analysis over music elements (e.g. pace), when used for short videos, interactive games, and entertainment applications, can help enrich content and campaign activities, and provide more interaction possibilities.\n*Audio/video editing: With time stamps of beats clearly identified, users can now create better beat-matching videos.\n*Demo\n* \n*\n*Music rhythm\n*Introduction\n*Detect and locate sudden changes in music for the design of music games and music visualization.\n*Business value\n*Wide range of application scenarios. Changes in music can be used as sound effects (drumbeats, gunshots and sounds for beat-matching), visual effects (animation, transitions and lighting designs), music game design (BeatSaber and Rhythm Master), and interactive changing points in mini games (gesture dances and catching eggs).\n*Make the use of music more interesting by matching body movements with music.\n*Make use of multidimensional information including rhythms, drumbeats and music structures. Diversify the use of music by matching visual effects and actions with changes in music.\n*Application scenario\n*Visual effects: apply the results of music pattern in scenarios such as animation, transitions and lighting designs to enrich visual effects. The function is launched on Douyin.\n*Stickers: create cool stickers that interact with facial and body movements.\n*Music games: help with the design of music games by matching changes in music with actions in games.\n*Music-based video editing: use multidimensional information to match with videos.\n*Demo\n* \n*\n*Climax detection\n*Introduction\n*With the help of algorithms, we can now detect one or more choruses in a song and determine which part is the most expressive. You can get the time range of the chorus (climax) by entering a piece of selected audio.\n*Business value\n*Climax plays a key role in expressing emotion, impressing users and lowering the threshold of content discovery. \n*Music climax can also be used in videos and other modals to improve the quality of content generation.\n*The length of climax can be customized. The algorithms can be used to set the required length of climax.\n*Various forms and all languages are supported, including instrumental music.\n*Climax is selected based on the content of lyrics so that full sentences are preserved.\n*Application scenario\n*Karaoke: detect the choruses of songs in the music library for scenarios such as the cold start of karaoke. Meanwhile, time information of musical components including choruses can be used as the reference for how to split parts in duet.\n*Trial listening: chorus detection can capture the most representative part of a song for trial listening. For example, on music players apps, users can use trail listening to check whether a search result is what they want.  \n*Audio and video editing: the technology can capture a selected part of a song with appropriate length that contains the chorus, so users don't have to do so manually. Moreover, audio/video editing tools and algorithms can refer to the timing of choruses for effects. \n*Demo\n* \n\n*Intelligent extension\n*Introduction\n*Capture the key segments of music to achieve seamless extension.\n*Business value\n*Lower the threshold for content generation and improve consumption experience. The algorithm enables users to extend music without using professional editing tools. \n*The original impression and style are preserved.\n*Various formats are supported. The algorithm can analyze music and extend it to the required length to match with the video, achieving full customization. \n*Music creation is no longer limited by the length of music pieces. Users now have a wider choices over usable audio material.\n*Applicable scenario\n*Music generation: generate music as required through intelligent extension. \n*Audio and video editing: intelligent extension is applied to a wide range of ByteDance's audio/video products. For example, Xigua's video creation tool adopts SAMI intelligent music extension technology, enriching the library of audio assets and giving users more freedom in music selection.\n*Video creation: extend music to match with the length of videos and complete video creation.\n*Ad creation: extend music to match with the length of ads.\n*Demo\n* \n*\n*Music Source Separation\n*Introduction\n*Music Source Separation (MSS) is a technology that separates audio into audio signals of different music sources. The primary function and application of MSS is to separate human voices from instruments. The algorithm can distinguish the audio track of human voice from the tracks of instruments. You can either store the results, or extract what you want based on specific scenarios.\n*Business value\n*Diversify music editing methods to increase the number of scenarios and flexibility of music asset application, lower the threshold for content creation and obtain music assets in an efficient and intelligent way. \n*Promote the efficiency of content material creation, such as accompaniment production and the cleaning of infringement content. \n*Separate clear human voice from instruments to expand application scenarios and inspire re-creation, so as to make full use of assets.\n*Application scenario\n*Audio/video editing: support the extraction of target human voice from other sounds in music and other audio materials, enabling further re-creations such as volume adjustment. \n*Karaoke: MSS can be used before pitch correction to separate, extract, and store tracks of voice and instruments in users' karaoke performances. The extracted instrumental tracks can be used as materials for karaoke apps. \n*Demo\n*Music source document\n* \n*Human voice separation\n* \n*Background music separation\n* \n*\n*Structure analysis\n*Introduction\n*Detect and locate different functional structures in music, such as prelude, verse, chorus and bridge.\n*Business value\n*Analyze music structure and extract functional segments that could be applied in different scenarios, making the use of music assets more flexible. \n*Locate and analyze music structures in fine granularity and multiple dimensions. Batch analysis is supported. \n*Application scenario\n*Audio/video editing: locate functional segments that can help with the transition of effects and the display of key content in audio/video editing. \n*Karaoke app: use time information of fine-grained music structures as the basis for part splitting to improve duet experience in karaoke apps. \n*Demo\n*\n*Video BGM\n*Introduction\n*Select appropriate BGM for videos.\n*Business value\n*Intelligent automatic matching lowers the threshold for content creation, enabling ordinary users to produce high-quality audio/video content. \n*Flexible settings of music libraries help with efficient and quick identification and matching.\n*Fine-grained matching and diverse asset choices further strengthen the role of music assets in videos.\n*Application scenario\n*Audio/video editing: scenarios such as in Xigua video.\n*Ad asset generation: select music that matches the content of ads.\n*\n*Demo\n* \n*\n*Text BGM\n*Introduction\n*Select appropriate BGM for images/texts. \n*Business value\n*Intelligent editing: lower the threshold for content creation and users don't have to spend much time searching for BGMs.\n*Inspire creation: help image/text creators and expand content scope for video creators.\n*Diversify promotion channels: provide more forms of user-friendly promotion channels for image/text creators.\n*Application scenario\n*Audio/video editing: text/image to videos.\n*Album video: provide BGM for album videos.\n*\n*Demo\n* \n*\n*Similar music recommendation\n*Introduction\n*Recommend music pieces with similar audio content.\n*Business value\n*Promote music content discovery and satisfy different users' specific needs for music consumption.\n*Flexible resource allocation that meets recommendation requirements of different scenarios such as gaming, ad promotion, social and entertainment. \n*AI music creation helps users to create customized music pieces to their own needs without having to worry about copyright limitations.\n*A wide range of scenarios that require diversification or customization of music can be supported by incorporating this feature.\n*Application scenario\n*Music playlist aggregation: already launched to music library platforms such as Resso.\n*Music recommendation: similar music recommendation service on audio/video platforms.\n*Social and entertainment: specific requirements for music of different scenarios including gaming, live-streaming and entertainment. \n*Monetization: music recommendation for monetization assets such as ads, breaking the restrictions of copyright to achieve expected effects. \n*\n*Demo\n* \n*\n*Audio to MIDI\n*Introduction\n*Multiple formats of audio can be converted to MIDI (a digital music format) that contains information such as musical notes, dynamics and length. Detailed MIDI files can help creators edit audios and videos with greater flexibility and more choices, enrich the experience of using musical creatives in different scenarios, and provide basic capabilities for intelligent music generation. \n*Application scenario\n*Karaoke: convert karaoke performances into MIDI files, optimizing singing experience for creators and users through information visualization. With the help of MIDI, users can refer to detailed music information when singing, which can improve the quality of their work. Moreover, MIDI can also be used for the adaptation of styles and instruments, providing more choices for Karaoke.\n*Music asset library (music asset processing): converting audio to MIDI enables users to edit their work more flexibly, enriching the scenarios of asset application, and provide basic capabilities for intelligent music generation. \n*Training: visualization-aided singing boosts the effect and efficiency of training.\n*Demo\n*Try this feature on the official website of SAMI: https://sami.bytedance.net/#/capability/Song2MidiDemo\n* \n*\n*Audio processing and understanding\n*PM   \n*Tech POC   \n*üí° Tips: What is audio processing and understanding? üí°\n*Audio processing supports the processing of audio effects in multiple dimensions and areas including noise control and audio enhancement, enabling diverse effects and voice beautification. Other capabilities relating to understanding audio contents are also available, such as converting audio to texts and semantic understanding, thereby improving efficiency for audio content understanding.\n*\n*Intelligent noise control and audio enhancement (3A)\n*Introduction\n*AEC, AGC and ANC are known as the 3A algorithms:\n*Acoustic Echo Cancelling (AEC) aims to cancel the echo signals that a mic receives from a speaker when they are both working, making sure that the useful signals are not disrupted by echo signals. AEC is usually applied to scenarios such as echo processing for mics and video conferences.\n*Automatic Gain Control (AGC) refers to automatic volume balancing during recording. In many teleconferencing cases, the volume of two sides does not match. The \"automatic enhancement control\" feature can balance the volume of both sides, making sure that each side can be heard clearly. \n*Active Noise Control (ANC), also known as Noise Cancellation and Noise Suppression.\n*By adopting 3A technologies, we try our best to remove noise from signals while minimizing distortion, so as to make voices clearer and improve audio quality. \n*Business value\n*Compared with traditional signal processing solutions, our self-developed AEC and ANC algorithms based on deep learning perform better at handling echoes and noises. Meanwhile, our algorithms can better ensure hi-fi reproduction of audios under scenarios of low signal-to-echo ratio and signal-to-noise ratio. The algorithms all support 16k and 44k bit rates as well as single/dual-track input. \n*For scenarios where echoes and noises both exist, we offer a deep learning solution that integrates echo cancelation and noise control, minimizing the difficulty of calculation by addressing two problems at once. \n*Customized quality optimization solutions for different scenarios: we have conducted targeted optimization for scenarios including real-time audio/video communication (conferencing and live-streaming), music, karaoke and audio/video editing. \n*Application scenario\n*Audio/video communication: real-time noise control facilitates better audio/video communication. We provide services for ByteDance products such as Feishu, and deliver targeted noise control services, for example, eliminating keyboard sounds during conferences.\n*Audio/video editing: real-time noise control to improve the sound quality of videos. The feature is launched for Douyin short videos.\n*Live-streaming: satisfy the requirements for music quality improvement for all types of live-streaming scenarios such as live-chatting and live-showrooms. For scenarios with existing bgm, we can control noise to the maximum without compromising human voice and bgm. The feature is launched for Douyin live.\n*Karaoke: customized solution for karaoke can remove reverberation in singing, achieving both noise control and hi-fi human voice enhancement. \n*\n*Demo\n*Noise control\n**\n*Echo cancellation\n**\n*Voice change + sound effect\n*Introduction\n*Modify parameters to change human voices into the minions, robots, etc. Various mutation effects are supported. \n*Interesting mutations that change the original voice (eg. the minions, robots)\n*Beautification without changing the original voice (eg. gentle female voice, attractive male voice)\n*Effects that simulate certain scenarios or atmospheres (eg. studio, concert)\n*Business value\n*Customization by modifying parameters is supported. Flexible and easy-to-use. \n*Voice changing delivers natural, fluent, and clear output.\n*Rea-time mutation with super-low latency is supported. \n*Flexible integration that supports mobile SDK and real-time API meets the requirements of different scenarios. \n*Application scenarios\n*Audio/video platforms: create various interesting voice change effects with a high similarity rate which are very entertaining. The feature is launched for Douyin editing page, Douyin stickers and Jianying. \n*Voice chat in games: clear, smooth, interesting and suitable for gaming scenarios (suits characters in games)\n*Social entertainment: scenarios that need interesting effects and beautification features, such as music radio, dubbing, and voice chat. \n*Karaoke: enrich karaoke experience via voice change without distortion, environment and atmosphere effects and beautification. The feature is launched on Douyin karaoke. \n*\n*Demo\n*Voice change in Douyin editing page: AudioSDK voice change demo\n \n*\n*Volume balancing\n*Introduction\n*Solve experience issues caused by the imbalance of volume between different files and parts, especially the problem with fluctuating volume. \n*Business value\n*Intelligent detection of sound features to determine the overall volume of the audio.\n*Real-time dynamic processing to adjust audio volume.\n*The feature is launched on products with high requirements for volume balancing and user experience such as Douyin and TikTok, and is proved to bring significant gains for key play metrics. \n*Application scenario\n*Audio/video play: ensure good user experience when the volume of audio and video does not match. The feature is launched on Douyin and TikTok.\n*Audio/video editing: boost editing efficiency and balance volume intelligently, thus inspiring and simplifying creation.\n*Multi-party chatting: scenarios such as audio/video conferencing and live-streaming. \n*\n*Demo\n*Take Douyin audio as an example\n**\n*\n*Spatial audio\n*Introduction\n*Spatial 3D audio production and play rendering.\n*Business value\n*Provide VR/AR immersive audio experience with surround sound effect, as if the user is really on the scene. \n*Give users a strong sense of space. The feature perceives spacial changes intelligently and adjusts sound field accordingly. \n*Application scenario \n*Audio/video play: create immersive spacial sound effects to bring significant improvement for sound experience.\n*Audios scenarios including Amazing engine \u0026 Effect studio and Resso.\n*\n*Demo\n* \n*\n*Voice recognition - basic Chinese and English\n*Introduction\n*Provide speech-to-text capability that converts audio/video into texts. \n*Business value\n*Combine ASR and OCR to provide scripts for informative oral endorsement videos. \n*Voice recognition technology based on deep learning ensures high accuracy.\n*Support voice recognition for various languages including Chinese and English with wide application range.\n*Intelligent voice text alignment that returns results with time stamps if users enter voice/music with texts/lyrics.\n*Understand context with help of pause identification for intelligent segmentation and punctuation. \n*Application scenario\n*Chatting: transcription of voice chats and voice search\n*Audio/video editing: convert audio of oral endorsement videos into scripts for intelligent editing. Provide filler removal capability for Xigua. Return voice identification results with time stamps at character level, enabling intelligent editing. \n*Entertainment and gaming: subtitle conversion for live-streaming and gaming.\n*Audio transcription: identify audio materials and convert them into texts in conferencing and other similar scenarios for display and storage.\n*\n*Demo\n* \n*\n*Filler editing \n*Introduction\n*Detect fillers in audio and return results with voice smoothness tags and time stamps. \n*Business value\n*Help creators identify and delete fillers to reduce editing time.\n*Creators can delete choppy contents through one click based on the identification results,  increasing information density. \n*Ensure the smoothness and quality of the edited video through processing the joints to avoid sudden changes. \n*Application scenario\n*Video editing: empower video creators to edit efficiently and inspire creation. The feature is launched on Xigua Video. \n*Speech processing: intelligent filler removal can significantly cut editing time and enable creators to focus on creation itself, especially for scenarios with high requirement for language quality such as radio and broadcasts. \n*\n*Demo\n*Xigua intelligent filler removal: remove pauses, lags and undesired mutes with one click and help users get their hands on editing.  \n* \n*ÁÆÄÈü≥.mp4 \n* \n*\n*Age identification\n*Introduction\n*Identify the age of speakers via voice detection.\n*Business value\n*Intelligent age moderation: identify age via voice detection. \n*Profile analysis: identify age for profile analysis.\n*Automatic identification: provide accurate age identification results based on deep learning.\n*Application scenario\n*Social interactions and entertainment: add personality tags to users based on the analysis of voice information for more accurate matching.\n*Customer service: provide voice information profiling for customers as a basis for user profiling.\n*Live gaming: help to meet requirements of risk monitoring and other scenarios with age identification.\n*\n*Demo\n*Sound event detection\n*Introduction\n*Detect special sound effects (animal sounds, gunshot, etc.)\n*Business value\n*Provide diverse sound tags as part of video information.\n*Support the analysis of audio in various formats.\n*Facilitate the entire audio and video editing process through accurate sound identification as a preparation step, laying the foundation for more extensive and precise application of music and sounds.\n*Application scenario\n*Editing: detect sound events which can be used to trigger certain actions to make creation more interesting. \n*Risk monitoring: detect certain sounds for risk monitoring.\n*Live-streaming: provide tagging models for live-steaming classification.\n*\n*Demo\n* \n*https://sami.bytedance.net/#/capability/AEDDemo\n*\n*Low-power wake-up\n*Introduction\n*Double-level wake-up on mobile ends. \n*\n*Business value\n*Customized service that supports a double-level wake-up plan with DIY wake-up words. \n*Offline wake-up is supported.\n*Intelligent detection of users' voices as the first step of voice chats. \n*Low-power continuous detection that enables wake-up triggered by voice.\n*High-accuracy, low-latency and real-time response.\n*Application scenario\n*Human-machine interaction: devices can be awakened by voice commands of users, improving experience of human-machine interaction. \n*Digital/virtual humans: wake-up service makes virtual humans more real, empowering them to communicate with users emotionally.\n*Voice assistant: wake up voice assistants when the device is sleeping or locked. \n*Intelligent hardware: toys, watches and intelligent furniture can respond to users' commands in a timely manner, optimizing user experience.\n*\n*Demo\n\n